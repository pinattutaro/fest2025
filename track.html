<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>リアルタイム姿勢推定クライアント</title>
  <style>
    video, canvas { display: block; margin: 0 auto; }
    #console {
        width: 400px;
        height: 100px;
        position: fixed;
        top: 0px;
        left: 0px;
        background-color: rgba(0, 0, 0, 0.5);
        color: white;
        font-family: monospace;
    }
  </style>
  <script src="https://docs.opencv.org/4.x/opencv.js"></script>
</head>
<body>
  <div id="console"></div>
  <video id="video" autoplay playsinline width="640" height="480"></video>
  <canvas id="canvas" width="640" height="480"></canvas>

  <script>
    let video = document.getElementById('video');
    let canvas = document.getElementById('canvas');
    let ctx = canvas.getContext('2d');

    let orb, bf;
    let firstKeypoints, firstDescriptors;
    let firstFrame = null;

    function round2(n) {
      return Math.round((n + Number.EPSILON) * 100) / 100;
    }

    async function initCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { width: 640, height: 480, facingMode: { ideal: "environment" } }
      });
      video.srcObject = stream;
      return new Promise(res => video.onloadedmetadata = res);
    }

    function initOpenCV() {
      orb = new cv.ORB();
      bf = new cv.BFMatcher(cv.NORM_HAMMING, true);

      firstKeypoints = new cv.KeyPointVector();
      firstDescriptors = new cv.Mat();
    }

    function captureFirstFrame(gray) {
      firstFrame = gray.clone();
      orb.detectAndCompute(firstFrame, new cv.Mat(), firstKeypoints, firstDescriptors);
      console.log("初期フレームキャプチャ完了");
    }

    function processFrame(gray) {
      if (!firstFrame) {
        captureFirstFrame(gray);
        return;
      }

      let kp = new cv.KeyPointVector();
      let des = new cv.Mat();
      orb.detectAndCompute(gray, new cv.Mat(), kp, des);

      let matches = new cv.DMatchVector();
      bf.match(firstDescriptors, des, matches);

      let points1 = [], points2 = [];
      for (let i = 0; i < matches.size(); i++) {
        let m = matches.get(i);
        points1.push({ x: firstKeypoints.get(m.queryIdx).pt.x, y: firstKeypoints.get(m.queryIdx).pt.y });
        points2.push({ x: kp.get(m.trainIdx).pt.x, y: kp.get(m.trainIdx).pt.y });
      }

      if (points1.length >= 8) {
        fetch("http://192.168.1.2:5000/pose", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ points1, points2 })
        })
        .then(res => res.json())
        .then(result => {
          if(result.success === false){
            console.warn(result.message);
          } else {
            console.log("回転行列:", result.R);
            console.log("並進ベクトル:", result.t);
            document.getElementById('console').innerText = `並進ベクトル:\n${(result.t).map(row => row.map(e => round2(e)))}\n回転行列:\n${(result.R).map(row => row.map(e => round2(e)))}`;
          }
        })
        .catch(err => console.error(err));
      }

      kp.delete(); des.delete(); matches.delete();
    }

    function mainLoop() {
      let frame = new cv.Mat(video.height, video.width, cv.CV_8UC4);
      ctx.drawImage(video, 0, 0, video.width, video.height);
      frame.data.set(ctx.getImageData(0, 0, video.width, video.height).data);

      let gray = new cv.Mat();
      cv.cvtColor(frame, gray, cv.COLOR_RGBA2GRAY);

      processFrame(gray);

      gray.delete(); frame.delete();
      requestAnimationFrame(mainLoop);
    }

    cv['onRuntimeInitialized'] = async () => {
      await initCamera();
      initOpenCV();
      mainLoop();
    };
  </script>
</body>
</html>
